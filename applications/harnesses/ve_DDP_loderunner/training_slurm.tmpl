#!/bin/bash

# This is a setup for GPU training on Venado. Find out how much
# memory per node, number of CPUs/node.

# NOTE: Number of CPUs per GPU must be an even number since there are
# 2 threads per core. If an odd number is requested the next higher
# even number gets used. 

# The following are one set of SBATCH options for the Venado GPU
# partition. There are optional other constraints.

#SBATCH --job-name=ddp_s<studyIDX>_e<epochIDX>
#SBATCH --account=w25_artimis_gh
#SBATCH --partition=gpu
#SBATCH --nodes=<KNODES>
#SBATCH --ntasks-per-node=<NGPUS>
#SBATCH --gpus-per-node=<NGPUS>
#SBATCH --cpus-per-task=70
#SBATCH --time=4:00:00
#SBATCH --output=study<studyIDX>_epoch<epochIDX>.out
#SBATCH --error=study<studyIDX>_epoch<epochIDX>.err
#SBATCH -vvv


# Set the master node's address and port
MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)
MASTER_PORT=$(shuf -i 1024-65535 -n 1)  # Choose a random port
export MASTER_ADDR MASTER_PORT

# Check available GPUs
sinfo  -o "%P %.24G %N"
srun /usr/bin/echo $CUDA_AVAILABLE_DEVICES

# Debugging distributed data parallel
export NCCL_DEBUG=INFO
export PYTHONFAULTHANDLER=1

# Load correct conda environment
module load python/3.10-anaconda-2023.03
source activate
conda activate <YOKE_TORCH_ENV>

# Calculate world size
WORLD_SIZE=$((SLURM_NNODES * SLURM_NTASKS_PER_NODE))

# Currently need to set an environment variable for MKL
# threading. Believed to be related to Numpy.
#export MKL_SERVICE_FORCE_INTEL=TRUE

# Get start time
export date00=`date`

# Start the Code
# Run the training script using torchrun
srun torchrun \
    --nnodes=$SLURM_NNODES \
    --nproc_per_node=$SLURM_NTASKS_PER_NODE \
    --rdzv_id=$SLURM_JOB_ID \
    --rdzv_backend=c10d \
    --rdzv_endpoint=$MASTER_ADDR:$MASTER_PORT \
    <train_script> @<INPUTFILE>

# Get end time and print to stdout
export date01=`date`

echo "===================TIME STARTED==================="
echo $date00
echo "===================TIME FINISHED==================="
echo $date01
